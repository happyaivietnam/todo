# Multi-layer perceptrons

A perceptron produces a single output based on several real-valued inputs by forming a linear combination using its input weights and passing the output through a nonlinear activation function. A perceptron is computed as a linear combination of all the nodes from the previous layers as
$y=\delta \left ( \sum_{0}^{n} w_ix_i \right ) = \delta \left ( w^Tx \right )$ where `w` denotes the vector of weights, `x` is the vector of inputs, $x_0 = 1$, $w_0$ is the bias weight, and \delta is the non-linear activation function.

\[
E = mc^2
\]
